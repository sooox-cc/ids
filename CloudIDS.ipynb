{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms for Network Intrusion Detection\n",
    "## Comparative Evaluation with Hyperparameter Optimization\n",
    "\n",
    "**Course:** SIM600 - Cloud Security  \n",
    "**Dataset:** UNSW-NB15  \n",
    "**Methodology:** Experimental Design with proper train/test separation\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology Overview\n",
    "\n",
    "This notebook implements a **rigorous experimental research design** following IEEE standards:\n",
    "\n",
    "1. **Data Loading**: Load pre-split train/test sets (maintain separation)\n",
    "2. **Preprocessing**: Fit transformations on train set, apply to both\n",
    "3. **Baseline Evaluation**: Evaluate default algorithms on train set with 5-fold CV\n",
    "4. **Hyperparameter Tuning**: Grid search on train set only using CV\n",
    "5. **Final Evaluation**: Test optimized models on independent test set\n",
    "6. **Results Export**: Generate publication-ready metrics and visualizations\n",
    "\n",
    "**Key Principle**: Test data is NEVER seen during training or hyperparameter tuning to ensure unbiased performance estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment configured for optimal performance\n"
     ]
    }
   ],
   "source": [
    "# Install Required Packages\n",
    "#%pip install -q seaborn psutil memory_profiler scikit-learn matplotlib pandas numpy\n",
    "\n",
    "# Memory optimization\n",
    "import gc\n",
    "import os\n",
    "\n",
    "gc.set_threshold(700, 10, 10)\n",
    "\n",
    "print(\"✅ Environment configured for optimal performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n",
      "\n",
      "======================================================================\n",
      "SYSTEM INFO\n",
      "======================================================================\n",
      "CPU Cores: 12\n",
      "Total RAM: 15.93 GB\n",
      "Available RAM: 6.37 GB\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import psutil\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"SYSTEM INFO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"CPU Cores: {os.cpu_count()}\")\n",
    "print(f\"Total RAM: {psutil.virtual_memory().total / (1024**3):.2f} GB\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / (1024**3):.2f} GB\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "**CRITICAL**: We load train and test sets separately and keep them separated throughout the entire pipeline.\n",
    "\n",
    "**Note**: The UNSW-NB15 files appear to have swapped labels:\n",
    "- UNSW_NB15_training-set.csv contains 82,332 samples (smaller set)\n",
    "- UNSW_NB15_testing-set.csv contains 175,341 samples (larger set)\n",
    "\n",
    "We'll use the larger file for training and smaller for testing, which is the standard practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UNSW-NB15 datasets...\n",
      "\n",
      "======================================================================\n",
      "DATASET LOADING COMPLETE\n",
      "======================================================================\n",
      "Training set: 175,341 samples, 45 features\n",
      "Test set:     82,332 samples, 45 features\n",
      "Total:        257,673 samples\n",
      "\n",
      "======================================================================\n",
      "CLASS DISTRIBUTION\n",
      "======================================================================\n",
      "\n",
      "Training Set:\n",
      "  Attack samples:  119,341 (68.06%)\n",
      "  Normal samples:  56,000 (31.94%)\n",
      "\n",
      "Test Set:\n",
      "  Attack samples:  45,332 (55.06%)\n",
      "  Normal samples:  37,000 (44.94%)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading UNSW-NB15 datasets...\\n\")\n",
    "\n",
    "# Load datasets - NOTE: Files appear to have swapped naming\n",
    "# Using larger set for training, smaller for testing\n",
    "train_raw = pd.read_csv('UNSW_NB15_testing-set.csv')  # 175,341 samples - LARGER\n",
    "test_raw = pd.read_csv('UNSW_NB15_training-set.csv')   # 82,332 samples - SMALLER\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET LOADING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training set: {train_raw.shape[0]:,} samples, {train_raw.shape[1]} features\")\n",
    "print(f\"Test set:     {test_raw.shape[0]:,} samples, {test_raw.shape[1]} features\")\n",
    "print(f\"Total:        {train_raw.shape[0] + test_raw.shape[0]:,} samples\")\n",
    "\n",
    "# Analyze class distribution\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_attacks = train_raw['label'].sum()\n",
    "train_normal = len(train_raw) - train_attacks\n",
    "test_attacks = test_raw['label'].sum()\n",
    "test_normal = len(test_raw) - test_attacks\n",
    "\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  Attack samples:  {train_attacks:>6,} ({train_attacks/len(train_raw)*100:>5.2f}%)\")\n",
    "print(f\"  Normal samples:  {train_normal:>6,} ({train_normal/len(train_raw)*100:>5.2f}%)\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  Attack samples:  {test_attacks:>6,} ({test_attacks/len(test_raw)*100:>5.2f}%)\")\n",
    "print(f\"  Normal samples:  {test_normal:>6,} ({test_normal/len(test_raw)*100:>5.2f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store original copies\n",
    "train_original = train_raw.copy()\n",
    "test_original = test_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "**Methodology**: \n",
    "1. Fit preprocessing on training set only\n",
    "2. Apply same transformations to both train and test\n",
    "3. This prevents data leakage from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PREPROCESSING PIPELINE WITH CLASS BALANCING (UNDERSAMPLING)\n",
      "======================================================================\n",
      "\n",
      "1. Removing ID and attack_cat columns...\n",
      "2. Identifying numeric columns for normalization...\n",
      "   Found 39 numeric columns to normalize\n",
      "3. Processing categorical features...\n",
      "   Found 3 categorical columns: ['proto', 'service', 'state']\n",
      "4. Performing one-hot encoding...\n",
      "   Shape after encoding: (175341, 195)\n",
      "5. Separating features and labels...\n",
      "6. Normalizing numeric features (fit on train only)...\n",
      "   Note: One-hot encoded features are already 0/1 and don't need normalization\n",
      "   Normalizing 39 numeric columns\n",
      "\n",
      "7. Balancing classes by downsampling majority class...\n",
      "\n",
      "   BEFORE BALANCING:\n",
      "   Training set - Class 0: 56000, Class 1: 119341\n",
      "   Test set     - Class 0: 37000, Class 1: 45332\n",
      "\n",
      "   AFTER BALANCING:\n",
      "   Training set - Class 0: 56000, Class 1: 56000\n",
      "   Test set     - Class 0: 37000, Class 1: 37000\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSING COMPLETE\n",
      "======================================================================\n",
      "Training features: (112000, 194)\n",
      "Training labels:   (112000,)\n",
      "Test features:     (74000, 194)\n",
      "Test labels:       (74000,)\n",
      "======================================================================\n",
      "\n",
      "✅ Data preprocessing complete. Train and test sets ready with balanced classes.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Preprocess train and test sets consistently with class balancing.\n",
    "    \n",
    "    IMPORTANT: All transformations are fit on train set only,\n",
    "    then applied to both train and test sets.\n",
    "    Classes are balanced using undersampling for both train and test.\n",
    "    \n",
    "    Returns:\n",
    "        X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREPROCESSING PIPELINE WITH CLASS BALANCING (UNDERSAMPLING)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Remove unnecessary columns\n",
    "    print(\"\\n1. Removing ID and attack_cat columns...\")\n",
    "    cols_to_drop = ['id', 'attack_cat']\n",
    "    train_clean = train_df.drop(cols_to_drop, axis=1)\n",
    "    test_clean = test_df.drop(cols_to_drop, axis=1)\n",
    "    \n",
    "    # Step 2: Identify numeric columns BEFORE encoding\n",
    "    print(\"2. Identifying numeric columns for normalization...\")\n",
    "    numeric_cols_before_encoding = train_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'label' in numeric_cols_before_encoding:\n",
    "        numeric_cols_before_encoding.remove('label')\n",
    "    print(f\"   Found {len(numeric_cols_before_encoding)} numeric columns to normalize\")\n",
    "    \n",
    "    # Step 3: Handle categorical variables\n",
    "    print(\"3. Processing categorical features...\")\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    cat_cols = train_clean.select_dtypes('object').columns.tolist()\n",
    "    print(f\"   Found {len(cat_cols)} categorical columns: {cat_cols}\")\n",
    "    \n",
    "    # Replace '-' with 'None' in categorical columns\n",
    "    for col in cat_cols:\n",
    "        train_clean[col] = train_clean[col].replace('-', 'None')\n",
    "        test_clean[col] = test_clean[col].replace('-', 'None')\n",
    "    \n",
    "    # One-hot encoding (fit on train, apply to both)\n",
    "    print(\"4. Performing one-hot encoding...\")\n",
    "    train_encoded = pd.get_dummies(train_clean, columns=cat_cols, drop_first=False)\n",
    "    test_encoded = pd.get_dummies(test_clean, columns=cat_cols, drop_first=False)\n",
    "    \n",
    "    # Align columns (test might have different categories)\n",
    "    train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)\n",
    "    \n",
    "    print(f\"   Shape after encoding: {train_encoded.shape}\")\n",
    "    \n",
    "    # Step 4: Separate features and labels\n",
    "    print(\"5. Separating features and labels...\")\n",
    "    X_train = train_encoded.drop('label', axis=1)\n",
    "    y_train = train_encoded['label'].astype(int)\n",
    "    X_test = test_encoded.drop('label', axis=1)\n",
    "    y_test = test_encoded['label'].astype(int)\n",
    "    \n",
    "    # Step 5: Normalize ONLY the original numeric features\n",
    "    print(\"6. Normalizing numeric features (fit on train only)...\")\n",
    "    print(\"   Note: One-hot encoded features are already 0/1 and don't need normalization\")\n",
    "    \n",
    "    # Identify which columns correspond to original numeric columns\n",
    "    numeric_cols_to_normalize = [col for col in X_train.columns if col in numeric_cols_before_encoding]\n",
    "    print(f\"   Normalizing {len(numeric_cols_to_normalize)} numeric columns\")\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    if len(numeric_cols_to_normalize) > 0:\n",
    "            \n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        # Fit on train, transform both (StandardScaler handles zero std internally)\n",
    "        X_train[numeric_cols_to_normalize] = scaler.fit_transform(X_train[numeric_cols_to_normalize])\n",
    "        X_test[numeric_cols_to_normalize] = scaler.transform(X_test[numeric_cols_to_normalize])\n",
    "        \n",
    "        # Safety check: Replace infinities and NaNs with 0 (unlikely but defensive)\n",
    "        X_train[numeric_cols_to_normalize] = X_train[numeric_cols_to_normalize].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "        X_test[numeric_cols_to_normalize] = X_test[numeric_cols_to_normalize].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "        \n",
    "        # Convert all columns to float for consistency\n",
    "        X_train_normalized = X_train.astype(float)\n",
    "        X_test_normalized = X_test.astype(float)\n",
    "        \n",
    "        # Step 6: Balance classes by downsampling majority class\n",
    "        print(\"\\n7. Balancing classes by downsampling majority class...\")\n",
    "        \n",
    "        # Check class distribution before balancing\n",
    "        print(\"\\n   BEFORE BALANCING:\")\n",
    "        train_class_0 = (y_train == 0).sum()\n",
    "        train_class_1 = (y_train == 1).sum()\n",
    "        test_class_0 = (y_test == 0).sum()\n",
    "        test_class_1 = (y_test == 1).sum()\n",
    "        print(f\"   Training set - Class 0: {train_class_0}, Class 1: {train_class_1}\")\n",
    "        print(f\"   Test set     - Class 0: {test_class_0}, Class 1: {test_class_1}\")\n",
    "    \n",
    "    # Balance training set\n",
    "    if train_class_0 > train_class_1:\n",
    "        # Class 0 is majority, downsample it to match class 1\n",
    "        minority_count = train_class_1\n",
    "        majority_indices = y_train[y_train == 0].index\n",
    "        minority_indices = y_train[y_train == 1].index\n",
    "        \n",
    "        # Randomly sample from majority class\n",
    "        np.random.seed(42)\n",
    "        downsampled_majority = np.random.choice(majority_indices, size=minority_count, replace=False)\n",
    "        \n",
    "        # Combine indices\n",
    "        balanced_indices = np.concatenate([downsampled_majority, minority_indices])\n",
    "        \n",
    "        X_train_balanced = X_train_normalized.iloc[balanced_indices]\n",
    "        y_train_balanced = y_train.iloc[balanced_indices]\n",
    "    else:\n",
    "        # Class 1 is majority, downsample it to match class 0\n",
    "        minority_count = train_class_0\n",
    "        majority_indices = y_train[y_train == 1].index\n",
    "        minority_indices = y_train[y_train == 0].index\n",
    "        \n",
    "        # Randomly sample from majority class\n",
    "        np.random.seed(42)\n",
    "        downsampled_majority = np.random.choice(majority_indices, size=minority_count, replace=False)\n",
    "        \n",
    "        # Combine indices\n",
    "        balanced_indices = np.concatenate([downsampled_majority, minority_indices])\n",
    "        \n",
    "        X_train_balanced = X_train_normalized.iloc[balanced_indices]\n",
    "        y_train_balanced = y_train.iloc[balanced_indices]\n",
    "    \n",
    "    # Balance test set\n",
    "    if test_class_0 > test_class_1:\n",
    "        # Class 0 is majority, downsample it to match class 1\n",
    "        minority_count = test_class_1\n",
    "        majority_indices = y_test[y_test == 0].index\n",
    "        minority_indices = y_test[y_test == 1].index\n",
    "        \n",
    "        # Randomly sample from majority class\n",
    "        np.random.seed(42)\n",
    "        downsampled_majority = np.random.choice(majority_indices, size=minority_count, replace=False)\n",
    "        \n",
    "        # Combine indices\n",
    "        balanced_indices = np.concatenate([downsampled_majority, minority_indices])\n",
    "        \n",
    "        X_test_balanced = X_test_normalized.iloc[balanced_indices]\n",
    "        y_test_balanced = y_test.iloc[balanced_indices]\n",
    "    else:\n",
    "        # Class 1 is majority, downsample it to match class 0\n",
    "        minority_count = test_class_0\n",
    "        majority_indices = y_test[y_test == 1].index\n",
    "        minority_indices = y_test[y_test == 0].index\n",
    "        \n",
    "        # Randomly sample from majority class\n",
    "        np.random.seed(42)\n",
    "        downsampled_majority = np.random.choice(majority_indices, size=minority_count, replace=False)\n",
    "        \n",
    "        # Combine indices\n",
    "        balanced_indices = np.concatenate([downsampled_majority, minority_indices])\n",
    "        \n",
    "        X_test_balanced = X_test_normalized.iloc[balanced_indices]\n",
    "        y_test_balanced = y_test.iloc[balanced_indices]\n",
    "    \n",
    "    # Check class distribution after balancing\n",
    "    print(\"\\n   AFTER BALANCING:\")\n",
    "    print(f\"   Training set - Class 0: {(y_train_balanced == 0).sum()}, Class 1: {(y_train_balanced == 1).sum()}\")\n",
    "    print(f\"   Test set     - Class 0: {(y_test_balanced == 0).sum()}, Class 1: {(y_test_balanced == 1).sum()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREPROCESSING COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Training features: {X_train_balanced.shape}\")\n",
    "    print(f\"Training labels:   {y_train_balanced.shape}\")\n",
    "    print(f\"Test features:     {X_test_balanced.shape}\")\n",
    "    print(f\"Test labels:       {y_test_balanced.shape}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return X_train_balanced, y_train_balanced, X_test_balanced, y_test_balanced\n",
    "\n",
    "# Preprocess data\n",
    "X_train, y_train, X_test, y_test = preprocess_data(train_original, test_original)\n",
    "\n",
    "# Force garbage collection\n",
    "del train_raw, test_raw, train_original, test_original\n",
    "gc.collect()\n",
    "\n",
    "print(\"✅ Data preprocessing complete. Train and test sets ready with balanced classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation Functions\n",
    "\n",
    "We define comprehensive evaluation functions that:\n",
    "1. Train models on training set\n",
    "2. Evaluate on both train (with CV) and test sets\n",
    "3. Calculate cloud deployment metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def get_model_size_mb(model):\n",
    "    \"\"\"Calculate model size in MB\"\"\"\n",
    "    try:\n",
    "        model_bytes = pickle.dumps(model)\n",
    "        return len(model_bytes) / (1024 * 1024)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def evaluate_model_comprehensive(model, X_train, y_train, X_test, y_test, \n",
    "                                  model_name=\"Model\", cv=2):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with proper train/test separation.\n",
    "    \n",
    "    Args:\n",
    "        model: Sklearn model instance\n",
    "        X_train, y_train: Training data\n",
    "        X_test, y_test: Test data (held-out, unseen)\n",
    "        model_name: Name for reporting\n",
    "        cv: Number of cross-validation folds for train evaluation\n",
    "    \n",
    "    Returns:\n",
    "        results dict with train and test metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATING: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = {'model_name': model_name}\n",
    "    \n",
    "    # Get baseline memory\n",
    "    process = psutil.Process(os.getpid())\n",
    "    baseline_memory = process.memory_info().rss / (1024 * 1024)\n",
    "    \n",
    "    # ========================================\n",
    "    # TRAINING PHASE\n",
    "    # ========================================\n",
    "    print(\"\\n[1/5] Training model on training set...\")\n",
    "    train_start = time.time()\n",
    "    \n",
    "    def train_model():\n",
    "        return model.fit(X_train, y_train)\n",
    "    \n",
    "    mem_usage = memory_usage((train_model,), interval=0.1, timeout=None)\n",
    "    trained_model = model.fit(X_train, y_train)\n",
    "    \n",
    "    train_time = time.time() - train_start\n",
    "    peak_memory = max(mem_usage)\n",
    "    memory_used = peak_memory - baseline_memory\n",
    "    \n",
    "    results['training_time_sec'] = round(train_time, 3)\n",
    "    results['memory_mb'] = round(memory_used, 2)\n",
    "    \n",
    "    # ========================================\n",
    "    # CROSS-VALIDATION ON TRAINING SET\n",
    "    # ========================================\n",
    "    print(f\"[2/5] Cross-validation on training set (cv={cv})...\")\n",
    "    cv_start = time.time()\n",
    "    \n",
    "    y_train_pred_cv = cross_val_predict(model, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "    \n",
    "    cv_time = time.time() - cv_start\n",
    "    results['cv_time_sec'] = round(cv_time, 3)\n",
    "    \n",
    "    # Calculate train metrics (cross-validated)\n",
    "    results['train_accuracy'] = round(metrics.accuracy_score(y_train, y_train_pred_cv) * 100, 2)\n",
    "    results['train_precision'] = round(metrics.precision_score(y_train, y_train_pred_cv, zero_division=0) * 100, 2)\n",
    "    results['train_recall'] = round(metrics.recall_score(y_train, y_train_pred_cv, zero_division=0) * 100, 2)\n",
    "    results['train_f1'] = round(metrics.f1_score(y_train, y_train_pred_cv, zero_division=0) * 100, 2)\n",
    "    \n",
    "    # Train confusion matrix\n",
    "    cm_train = metrics.confusion_matrix(y_train, y_train_pred_cv)\n",
    "    tn_train, fp_train, fn_train, tp_train = cm_train.ravel()\n",
    "    results['train_tn'] = int(tn_train)\n",
    "    results['train_fp'] = int(fp_train)\n",
    "    results['train_fn'] = int(fn_train)\n",
    "    results['train_tp'] = int(tp_train)\n",
    "    results['train_fpr'] = round((fp_train / (fp_train + tn_train) * 100) if (fp_train + tn_train) > 0 else 0, 2)\n",
    "    \n",
    "    # ========================================\n",
    "    # TEST SET EVALUATION (CRITICAL)\n",
    "    # ========================================\n",
    "    print(\"[3/5] Evaluating on independent test set...\")\n",
    "    test_start = time.time()\n",
    "    \n",
    "    y_test_pred = trained_model.predict(X_test)\n",
    "    \n",
    "    test_time = time.time() - test_start\n",
    "    results['test_time_sec'] = round(test_time, 3)\n",
    "    results['avg_latency_ms'] = round((test_time / len(X_test)) * 1000, 4)\n",
    "    \n",
    "    # Calculate test metrics\n",
    "    results['test_accuracy'] = round(metrics.accuracy_score(y_test, y_test_pred) * 100, 2)\n",
    "    results['test_precision'] = round(metrics.precision_score(y_test, y_test_pred, zero_division=0) * 100, 2)\n",
    "    results['test_recall'] = round(metrics.recall_score(y_test, y_test_pred, zero_division=0) * 100, 2)\n",
    "    results['test_f1'] = round(metrics.f1_score(y_test, y_test_pred, zero_division=0) * 100, 2)\n",
    "    \n",
    "    # Test confusion matrix\n",
    "    cm_test = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "    tn_test, fp_test, fn_test, tp_test = cm_test.ravel()\n",
    "    results['test_tn'] = int(tn_test)\n",
    "    results['test_fp'] = int(fp_test)\n",
    "    results['test_fn'] = int(fn_test)\n",
    "    results['test_tp'] = int(tp_test)\n",
    "    results['test_fpr'] = round((fp_test / (fp_test + tn_test) * 100) if (fp_test + tn_test) > 0 else 0, 2)\n",
    "    \n",
    "    # Store confusion matrices\n",
    "    results['cm_train'] = cm_train\n",
    "    results['cm_test'] = cm_test\n",
    "    \n",
    "    # ========================================\n",
    "    # CLOUD DEPLOYMENT METRICS\n",
    "    # ========================================\n",
    "    print(\"[4/5] Calculating deployment metrics...\")\n",
    "    \n",
    "    model_size = get_model_size_mb(trained_model)\n",
    "    results['model_size_mb'] = round(model_size, 2)\n",
    "    \n",
    "    throughput = len(X_test) / test_time if test_time > 0 else 0\n",
    "    results['throughput_samples_per_sec'] = round(throughput, 2)\n",
    "    \n",
    "    # Cost-effectiveness: F1-score per second of training time\n",
    "    cost_effectiveness = results['test_f1'] / train_time if train_time > 0 else 0\n",
    "    results['cost_effectiveness'] = round(cost_effectiveness, 2)\n",
    "    \n",
    "    # Performance per MB: F1-score per MB of memory\n",
    "    perf_per_mb = results['test_f1'] / memory_used if memory_used > 0 else 0\n",
    "    results['performance_per_mb'] = round(perf_per_mb, 2)\n",
    "    \n",
    "    # Scalability score\n",
    "    avg_latency = results['avg_latency_ms']\n",
    "    scalability = (throughput / 1000) / avg_latency if avg_latency > 0 else 0\n",
    "    results['scalability_score'] = round(scalability, 4)\n",
    "    \n",
    "    # ========================================\n",
    "    # RESULTS SUMMARY\n",
    "    # ========================================\n",
    "    print(\"[5/5] Results compiled.\\n\")\n",
    "    \n",
    "    print(f\"{'TRAINING SET PERFORMANCE (Cross-Validated)':^70}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    print(f\"  Accuracy:        {results['train_accuracy']:>6.2f}%\")\n",
    "    print(f\"  Precision:       {results['train_precision']:>6.2f}%\")\n",
    "    print(f\"  Recall:          {results['train_recall']:>6.2f}%\")\n",
    "    print(f\"  F1-Score:        {results['train_f1']:>6.2f}%\")\n",
    "    print(f\"  False Pos. Rate: {results['train_fpr']:>6.2f}%\")\n",
    "    \n",
    "    print(f\"\\n{'TEST SET PERFORMANCE (Independent Evaluation)':^70}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    print(f\"  Accuracy:        {results['test_accuracy']:>6.2f}%\")\n",
    "    print(f\"  Precision:       {results['test_precision']:>6.2f}%\")\n",
    "    print(f\"  Recall:          {results['test_recall']:>6.2f}%\")\n",
    "    print(f\"  F1-Score:        {results['test_f1']:>6.2f}%\")\n",
    "    print(f\"  False Pos. Rate: {results['test_fpr']:>6.2f}%\")\n",
    "    \n",
    "    print(f\"\\n{'RESOURCE & DEPLOYMENT METRICS':^70}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    print(f\"  Training Time:   {results['training_time_sec']:>8.3f} sec\")\n",
    "    print(f\"  Test Time:       {results['test_time_sec']:>8.3f} sec\")\n",
    "    print(f\"  Avg Latency:     {results['avg_latency_ms']:>8.4f} ms/sample\")\n",
    "    print(f\"  Memory Used:     {results['memory_mb']:>8.2f} MB\")\n",
    "    print(f\"  Model Size:      {results['model_size_mb']:>8.2f} MB\")\n",
    "    print(f\"  Throughput:      {results['throughput_samples_per_sec']:>8.2f} samples/sec\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results, trained_model\n",
    "\n",
    "print(\"✅ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Model Evaluation\n",
    "\n",
    "Evaluate three algorithms with default hyperparameters to establish baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BASELINE EVALUATIONS (Default Parameters)\n",
      "======================================================================\n",
      "\n",
      "Evaluating models with 5-fold cross-validation on training set,\n",
      "then testing on independent test set.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE EVALUATIONS (Default Parameters)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nEvaluating models with 5-fold cross-validation on training set,\")\n",
    "print(\"then testing on independent test set.\\n\")\n",
    "\n",
    "baseline_results = {}\n",
    "baseline_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATING: Logistic Regression (Baseline)\n",
      "======================================================================\n",
      "\n",
      "[1/5] Training model on training set...\n",
      "[2/5] Cross-validation on training set (cv=2)...\n",
      "[3/5] Evaluating on independent test set...\n",
      "[4/5] Calculating deployment metrics...\n",
      "[5/5] Results compiled.\n",
      "\n",
      "              TRAINING SET PERFORMANCE (Cross-Validated)              \n",
      "----------------------------------------------------------------------\n",
      "  Accuracy:         85.44%\n",
      "  Precision:        80.32%\n",
      "  Recall:           93.89%\n",
      "  F1-Score:         86.57%\n",
      "  False Pos. Rate:  23.01%\n",
      "\n",
      "            TEST SET PERFORMANCE (Independent Evaluation)             \n",
      "----------------------------------------------------------------------\n",
      "  Accuracy:         82.41%\n",
      "  Precision:        76.66%\n",
      "  Recall:           93.20%\n",
      "  F1-Score:         84.12%\n",
      "  False Pos. Rate:  28.38%\n",
      "\n",
      "                    RESOURCE & DEPLOYMENT METRICS                     \n",
      "----------------------------------------------------------------------\n",
      "  Training Time:      8.592 sec\n",
      "  Test Time:          0.045 sec\n",
      "  Avg Latency:       0.0006 ms/sample\n",
      "  Memory Used:       182.86 MB\n",
      "  Model Size:          0.00 MB\n",
      "  Throughput:      1661368.35 samples/sec\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Logistic Regression (Baseline)\n",
    "import gc\n",
    "lr_baseline = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "results_lr_base, model_lr_base = evaluate_model_comprehensive(\n",
    "    lr_baseline, X_train, y_train, X_test, y_test,\n",
    "    model_name=\"Logistic Regression (Baseline)\", cv=5\n",
    ")\n",
    "baseline_results['Logistic Regression'] = results_lr_base\n",
    "baseline_models['Logistic Regression'] = model_lr_base\n",
    "\n",
    "gc.collect()  # Clean memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATING: K-Nearest Neighbors (Baseline)\n",
      "======================================================================\n",
      "\n",
      "[1/5] Training model on training set...\n",
      "[2/5] Cross-validation on training set (cv=2)...\n",
      "[3/5] Evaluating on independent test set...\n",
      "[4/5] Calculating deployment metrics...\n",
      "[5/5] Results compiled.\n",
      "\n",
      "              TRAINING SET PERFORMANCE (Cross-Validated)              \n",
      "----------------------------------------------------------------------\n",
      "  Accuracy:         85.40%\n",
      "  Precision:        80.60%\n",
      "  Recall:           93.24%\n",
      "  F1-Score:         86.46%\n",
      "  False Pos. Rate:  22.44%\n",
      "\n",
      "            TEST SET PERFORMANCE (Independent Evaluation)             \n",
      "----------------------------------------------------------------------\n",
      "  Accuracy:         86.04%\n",
      "  Precision:        81.84%\n",
      "  Recall:           92.65%\n",
      "  F1-Score:         86.91%\n",
      "  False Pos. Rate:  20.56%\n",
      "\n",
      "                    RESOURCE & DEPLOYMENT METRICS                     \n",
      "----------------------------------------------------------------------\n",
      "  Training Time:      1.526 sec\n",
      "  Test Time:         31.205 sec\n",
      "  Avg Latency:       0.4217 ms/sample\n",
      "  Memory Used:       492.50 MB\n",
      "  Model Size:        166.63 MB\n",
      "  Throughput:       2371.45 samples/sec\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. K-Nearest Neighbors (Baseline)\n",
    "knn_baseline = KNeighborsClassifier(n_jobs=-1)\n",
    "results_knn_base, model_knn_base = evaluate_model_comprehensive(\n",
    "    knn_baseline, X_train, y_train, X_test, y_test,\n",
    "    model_name=\"K-Nearest Neighbors (Baseline)\", cv=5\n",
    ")\n",
    "baseline_results['KNN'] = results_knn_base\n",
    "baseline_models['KNN'] = model_knn_base\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATING: Linear SVC (Baseline)\n",
      "======================================================================\n",
      "\n",
      "[1/5] Training model on training set...\n",
      "[2/5] Cross-validation on training set (cv=5)...\n",
      "[3/5] Evaluating on independent test set...\n",
      "[4/5] Calculating deployment metrics...\n",
      "[5/5] Results compiled.\n",
      "\n",
      "              TRAINING SET PERFORMANCE (Cross-Validated)              \n",
      "----------------------------------------------------------------------\n",
      "  Accuracy:         88.52%\n",
      "  Precision:        83.91%\n",
      "  Recall:           95.33%\n",
      "  F1-Score:         89.26%\n",
      "  False Pos. Rate:  18.28%\n",
      "\n",
      "            TEST SET PERFORMANCE (Independent Evaluation)             \n",
      "----------------------------------------------------------------------\n",
      "  Accuracy:         81.73%\n",
      "  Precision:        75.74%\n",
      "  Recall:           93.37%\n",
      "  F1-Score:         83.64%\n",
      "  False Pos. Rate:  29.90%\n",
      "\n",
      "                    RESOURCE & DEPLOYMENT METRICS                     \n",
      "----------------------------------------------------------------------\n",
      "  Training Time:      9.694 sec\n",
      "  Test Time:          0.040 sec\n",
      "  Avg Latency:       0.0005 ms/sample\n",
      "  Memory Used:       248.25 MB\n",
      "  Model Size:          0.00 MB\n",
      "  Throughput:      1848712.52 samples/sec\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Linear SVC (Baseline)\n",
    "svc_baseline = LinearSVC(max_iter=1000, random_state=42)\n",
    "results_svc_base, model_svc_base = evaluate_model_comprehensive(\n",
    "    svc_baseline, X_train, y_train, X_test, y_test,\n",
    "    model_name=\"Linear SVC (Baseline)\", cv=5\n",
    ")\n",
    "baseline_results['Linear SVC'] = results_svc_base\n",
    "baseline_models['Linear SVC'] = model_svc_base\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Optimization\n",
    "\n",
    "**Grid Search on Training Set Only** using 5-fold cross-validation.\n",
    "\n",
    "The test set is NOT used during hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Grid search function defined\n"
     ]
    }
   ],
   "source": [
    "def grid_search_optimize(model, param_grid, X_train, y_train, \n",
    "                         model_name=\"Model\", cv=5, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Perform grid search on TRAINING set only.\n",
    "    \n",
    "    CRITICAL: Test set is never used here.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"GRID SEARCH: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Parameter grid: {param_grid}\")\n",
    "    print(f\"Cross-validation folds: {cv}\")\n",
    "    print(f\"Scoring metric: F1-Score\")\n",
    "    print(f\"\\nSearching for optimal hyperparameters...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    # Fit on TRAINING set only\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"GRID SEARCH COMPLETE - {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Search time: {search_time:.2f} seconds\")\n",
    "    print(f\"\\nBest Parameters:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"\\nBest CV F1-Score (Train): {grid_search.best_score_*100:.2f}%\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "print(\"✅ Grid search function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Optimize Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HYPERPARAMETER TUNING: LOGISTIC REGRESSION\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "GRID SEARCH: Logistic Regression\n",
      "======================================================================\n",
      "Parameter grid: {'C': [100], 'penalty': ['l2'], 'solver': ['liblinear'], 'max_iter': [1000]}\n",
      "Cross-validation folds: 5\n",
      "Scoring metric: F1-Score\n",
      "\n",
      "Searching for optimal hyperparameters...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "======================================================================\n",
      "GRID SEARCH COMPLETE - Logistic Regression\n",
      "======================================================================\n",
      "Search time: 41.13 seconds\n",
      "\n",
      "Best Parameters:\n",
      "  C: 100\n",
      "  max_iter: 1000\n",
      "  penalty: l2\n",
      "  solver: liblinear\n",
      "\n",
      "Best CV F1-Score (Train): 90.65%\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EVALUATING: Logistic Regression (Tuned)\n",
      "======================================================================\n",
      "\n",
      "[1/5] Training model on training set...\n",
      "[2/5] Cross-validation on training set (cv=5)...\n",
      "[3/5] Evaluating on independent test set...\n",
      "[4/5] Calculating deployment metrics...\n",
      "[5/5] Results compiled.\n",
      "\n",
      "              TRAINING SET PERFORMANCE (Cross-Validated)              \n",
      "----------------------------------------------------------------------\n",
      "  Accuracy:         89.09%\n",
      "  Precision:        84.91%\n",
      "  Recall:           95.07%\n",
      "  F1-Score:         89.70%\n",
      "  False Pos. Rate:  16.89%\n",
      "\n",
      "            TEST SET PERFORMANCE (Independent Evaluation)             \n",
      "----------------------------------------------------------------------\n",
      "  Accuracy:         82.85%\n",
      "  Precision:        77.18%\n",
      "  Recall:           93.30%\n",
      "  F1-Score:         84.48%\n",
      "  False Pos. Rate:  27.59%\n",
      "\n",
      "                    RESOURCE & DEPLOYMENT METRICS                     \n",
      "----------------------------------------------------------------------\n",
      "  Training Time:     28.167 sec\n",
      "  Test Time:          0.040 sec\n",
      "  Avg Latency:       0.0005 ms/sample\n",
      "  Memory Used:       251.45 MB\n",
      "  Model Size:          0.00 MB\n",
      "  Throughput:      1844208.80 samples/sec\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SAVING MODEL\n",
      "======================================================================\n",
      "✓ Model saved: 123Logistic.pkl\n",
      "File size: 0.0047 MB\n",
      "Best parameters: {'C': 100, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best CV score: 0.9065\n",
      "\n",
      "--- VERIFYING SAVED MODEL ---\n",
      "Loaded model test accuracy: 0.8285\n",
      "✓ Model successfully saved and verified!\n",
      "\n",
      "======================================================================\n",
      "PROCESS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING: LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "lr_param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'solver': ['liblinear', 'SAGA', 'SAG'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "# Grid search on TRAINING set only\n",
    "lr_tuned, lr_best_params, lr_best_cv_score = grid_search_optimize(\n",
    "    model=LogisticRegression(random_state=42, n_jobs=-1),\n",
    "    param_grid=lr_param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    model_name=\"Logistic Regression\",\n",
    "    cv=5\n",
    ")\n",
    "# Evaluate on BOTH train and test\n",
    "results_lr_tuned, model_lr_tuned = evaluate_model_comprehensive(\n",
    "    lr_tuned, X_train, y_train, X_test, y_test,\n",
    "    model_name=\"Logistic Regression (Tuned)\", cv=5\n",
    ")\n",
    "model_filename = '123Logistic.pkl'\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*70)\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(model_lr_tuned, file)  # ✓ FIXED: Changed from model_knn_tuned\n",
    "# Get file size\n",
    "file_size_mb = os.path.getsize(model_filename) / (1024 * 1024)\n",
    "print(f\"✓ Model saved: {model_filename}\")\n",
    "print(f\"File size: {file_size_mb:.4f} MB\")\n",
    "print(f\"Best parameters: {lr_best_params}\")  # ✓ FIXED: Changed from knn_best_params\n",
    "print(f\"Best CV score: {lr_best_cv_score:.4f}\")  # ✓ FIXED: Changed from knn_best_cv_score\n",
    "# Verify the saved model\n",
    "print(\"\\n--- VERIFYING SAVED MODEL ---\")\n",
    "with open(model_filename, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "loaded_accuracy = accuracy_score(y_test, y_pred_loaded)\n",
    "print(f\"Loaded model test accuracy: {loaded_accuracy:.4f}\")\n",
    "print(\"✓ Model successfully saved and verified!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROCESS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Optimize K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HYPERPARAMETER TUNING: K-NEAREST NEIGHBORS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "GRID SEARCH: K-Nearest Neighbors\n",
      "======================================================================\n",
      "Parameter grid: {'n_neighbors': [11], 'weights': ['distance'], 'metric': ['manhattan'], 'algorithm': ['auto']}\n",
      "Cross-validation folds: 5\n",
      "Scoring metric: F1-Score\n",
      "\n",
      "Searching for optimal hyperparameters...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "======================================================================\n",
      "GRID SEARCH COMPLETE - K-Nearest Neighbors\n",
      "======================================================================\n",
      "Search time: 690.70 seconds\n",
      "\n",
      "Best Parameters:\n",
      "  algorithm: auto\n",
      "  metric: manhattan\n",
      "  n_neighbors: 11\n",
      "  weights: distance\n",
      "\n",
      "Best CV F1-Score (Train): 90.46%\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EVALUATING: K-Nearest Neighbors (Tuned)\n",
      "======================================================================\n",
      "\n",
      "[1/5] Training model on training set...\n",
      "[2/5] Cross-validation on training set (cv=2)...\n",
      "[3/5] Evaluating on independent test set...\n",
      "[4/5] Calculating deployment metrics...\n",
      "[5/5] Results compiled.\n",
      "\n",
      "              TRAINING SET PERFORMANCE (Cross-Validated)              \n",
      "----------------------------------------------------------------------\n",
      "  Accuracy:         85.69%\n",
      "  Precision:        80.75%\n",
      "  Recall:           93.71%\n",
      "  F1-Score:         86.75%\n",
      "  False Pos. Rate:  22.33%\n",
      "\n",
      "            TEST SET PERFORMANCE (Independent Evaluation)             \n",
      "----------------------------------------------------------------------\n",
      "  Accuracy:         87.68%\n",
      "  Precision:        83.80%\n",
      "  Recall:           93.43%\n",
      "  F1-Score:         88.35%\n",
      "  False Pos. Rate:  18.06%\n",
      "\n",
      "                    RESOURCE & DEPLOYMENT METRICS                     \n",
      "----------------------------------------------------------------------\n",
      "  Training Time:      1.434 sec\n",
      "  Test Time:        301.635 sec\n",
      "  Avg Latency:       4.0761 ms/sample\n",
      "  Memory Used:       304.19 MB\n",
      "  Model Size:        166.63 MB\n",
      "  Throughput:        245.33 samples/sec\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SAVING MODEL\n",
      "======================================================================\n",
      "✓ Model saved: 123knn_tuned_model.pkl\n",
      "File size: 166.6291 MB\n",
      "Best parameters: {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 11, 'weights': 'distance'}\n",
      "Best CV score: 0.9046\n",
      "\n",
      "--- VERIFYING SAVED MODEL ---\n",
      "Loaded model test accuracy: 0.8768\n",
      "✓ Model successfully saved and verified!\n",
      "\n",
      "======================================================================\n",
      "PROCESS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING: K-NEAREST NEIGHBORS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3,5,7,9,11],\n",
    "    'weights': ['distance', 'uniform'],\n",
    "    'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
    "}\n",
    "\n",
    "# Grid search on TRAINING set only\n",
    "knn_tuned, knn_best_params, knn_best_cv_score = grid_search_optimize(\n",
    "    model=KNeighborsClassifier(n_jobs=-1),\n",
    "    param_grid=knn_param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    model_name=\"K-Nearest Neighbors\",\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Evaluate on BOTH train and test\n",
    "results_knn_tuned, model_knn_tuned = evaluate_model_comprehensive(\n",
    "    knn_tuned, X_train, y_train, X_test, y_test,\n",
    "    model_name=\"K-Nearest Neighbors (Tuned)\", cv=5\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model_filename = '123knn_tuned_model.pkl'\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(model_knn_tuned, file)\n",
    "\n",
    "# Get file size\n",
    "file_size_mb = os.path.getsize(model_filename) / (1024 * 1024)\n",
    "\n",
    "print(f\"✓ Model saved: {model_filename}\")\n",
    "print(f\"File size: {file_size_mb:.4f} MB\")\n",
    "print(f\"Best parameters: {knn_best_params}\")\n",
    "print(f\"Best CV score: {knn_best_cv_score:.4f}\")\n",
    "\n",
    "# Verify the saved model\n",
    "print(\"\\n--- VERIFYING SAVED MODEL ---\")\n",
    "with open(model_filename, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "loaded_accuracy = accuracy_score(y_test, y_pred_loaded)\n",
    "print(f\"Loaded model test accuracy: {loaded_accuracy:.4f}\")\n",
    "print(\"✓ Model successfully saved and verified!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROCESS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Optimize Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HYPERPARAMETER TUNING: LINEAR SVC\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "GRID SEARCH: Linear SVC\n",
      "======================================================================\n",
      "Parameter grid: {'C': [100], 'penalty': ['l2'], 'loss': ['hinge'], 'max_iter': [1000]}\n",
      "Cross-validation folds: 5\n",
      "Scoring metric: F1-Score\n",
      "\n",
      "Searching for optimal hyperparameters...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "======================================================================\n",
      "GRID SEARCH COMPLETE - Linear SVC\n",
      "======================================================================\n",
      "Search time: 45.46 seconds\n",
      "\n",
      "Best Parameters:\n",
      "  C: 100\n",
      "  loss: hinge\n",
      "  max_iter: 1000\n",
      "  penalty: l2\n",
      "\n",
      "Best CV F1-Score (Train): 85.65%\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EVALUATING: Linear SVC (Tuned)\n",
      "======================================================================\n",
      "\n",
      "[1/5] Training model on training set...\n",
      "[2/5] Cross-validation on training set (cv=5)...\n",
      "[3/5] Evaluating on independent test set...\n",
      "[4/5] Calculating deployment metrics...\n",
      "[5/5] Results compiled.\n",
      "\n",
      "              TRAINING SET PERFORMANCE (Cross-Validated)              \n",
      "----------------------------------------------------------------------\n",
      "  Accuracy:         85.39%\n",
      "  Precision:        85.12%\n",
      "  Recall:           85.78%\n",
      "  F1-Score:         85.45%\n",
      "  False Pos. Rate:  14.99%\n",
      "\n",
      "            TEST SET PERFORMANCE (Independent Evaluation)             \n",
      "----------------------------------------------------------------------\n",
      "  Accuracy:         87.11%\n",
      "  Precision:        91.94%\n",
      "  Recall:           81.35%\n",
      "  F1-Score:         86.32%\n",
      "  False Pos. Rate:   7.13%\n",
      "\n",
      "                    RESOURCE & DEPLOYMENT METRICS                     \n",
      "----------------------------------------------------------------------\n",
      "  Training Time:     35.976 sec\n",
      "  Test Time:          0.047 sec\n",
      "  Avg Latency:       0.0006 ms/sample\n",
      "  Memory Used:       254.02 MB\n",
      "  Model Size:          0.00 MB\n",
      "  Throughput:      1572699.29 samples/sec\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SAVING MODEL\n",
      "======================================================================\n",
      "✓ Model saved: 123SVC.pkl\n",
      "File size: 0.0046 MB\n",
      "Best parameters: {'C': 100, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l2'}\n",
      "Best CV score: 0.8565\n",
      "\n",
      "--- VERIFYING SAVED MODEL ---\n",
      "Loaded model test accuracy: 0.8711\n",
      "✓ Model successfully saved and verified!\n",
      "\n",
      "======================================================================\n",
      "PROCESS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING: LINEAR SVC\")\n",
    "print(\"=\"*70)\n",
    "svc_param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10 100],\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "# Grid search on TRAINING set only\n",
    "svc_tuned, svc_best_params, svc_best_cv_score = grid_search_optimize(\n",
    "    model=LinearSVC(random_state=42),\n",
    "    param_grid=svc_param_grid,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    model_name=\"Linear SVC\",\n",
    "    cv=5\n",
    ")\n",
    "# Evaluate on BOTH train and test\n",
    "results_svc_tuned, model_svc_tuned = evaluate_model_comprehensive(\n",
    "    svc_tuned, X_train, y_train, X_test, y_test,\n",
    "    model_name=\"Linear SVC (Tuned)\", cv=5\n",
    ")\n",
    "model_filename = '123SVC.pkl'\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*70)\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(model_svc_tuned, file)  # ✓ FIXED: Changed from model_knn_tuned\n",
    "# Get file size\n",
    "file_size_mb = os.path.getsize(model_filename) / (1024 * 1024)\n",
    "print(f\"✓ Model saved: {model_filename}\")\n",
    "print(f\"File size: {file_size_mb:.4f} MB\")\n",
    "print(f\"Best parameters: {svc_best_params}\")  # ✓ FIXED: Changed from knn_best_params\n",
    "print(f\"Best CV score: {svc_best_cv_score:.4f}\")  # ✓ FIXED: Changed from knn_best_cv_score\n",
    "# Verify the saved model\n",
    "print(\"\\n--- VERIFYING SAVED MODEL ---\")\n",
    "with open(model_filename, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "loaded_accuracy = accuracy_score(y_test, y_pred_loaded)\n",
    "print(f\"Loaded model test accuracy: {loaded_accuracy:.4f}\")\n",
    "print(\"✓ Model successfully saved and verified!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROCESS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "COMPREHENSIVE RESULTS SUMMARY\n",
      "====================================================================================================\n",
      "\n",
      "                         Model  Train_Accuracy  Train_Precision  Train_Recall  Train_F1  Train_FPR  Test_Accuracy  Test_Precision  Test_Recall  Test_F1  Test_FPR  Training_Time_sec  Test_Time_sec  Avg_Latency_ms  Memory_MB  Model_Size_MB  Throughput  Cost_Effectiveness  Performance_per_MB\n",
      "Logistic Regression (Baseline)           85.44            80.32         93.89     86.57      23.01          82.41           76.66        93.20    84.12     28.38              8.592          0.045          0.0006     182.86           0.00  1661368.35                9.79                0.46\n",
      "   Logistic Regression (Tuned)           89.09            84.91         95.07     89.70      16.89          82.85           77.18        93.30    84.48     27.59             28.167          0.040          0.0005     251.45           0.00  1844208.80                3.00                0.34\n",
      "                KNN (Baseline)           85.40            80.60         93.24     86.46      22.44          86.04           81.84        92.65    86.91     20.56              1.526         31.205          0.4217     492.50         166.63     2371.45               56.95                0.18\n",
      "                   KNN (Tuned)           85.69            80.75         93.71     86.75      22.33          87.68           83.80        93.43    88.35     18.06              1.434        301.635          4.0761     304.19         166.63      245.33               61.61                0.29\n",
      "         Linear SVC (Baseline)           88.52            83.91         95.33     89.26      18.28          81.73           75.74        93.37    83.64     29.90              9.694          0.040          0.0005     248.25           0.00  1848712.52                8.63                0.34\n",
      "            Linear SVC (Tuned)           85.39            85.12         85.78     85.45      14.99          87.11           91.94        81.35    86.32      7.13             35.976          0.047          0.0006     254.02           0.00  1572699.29                2.40                0.34\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compile all results\n",
    "all_results = {\n",
    "    'Logistic Regression (Baseline)': results_lr_base,\n",
    "    'Logistic Regression (Tuned)': results_lr_tuned,\n",
    "    'KNN (Baseline)': results_knn_base,\n",
    "    'KNN (Tuned)': results_knn_tuned,\n",
    "    'Linear SVC (Baseline)': results_svc_base,\n",
    "    'Linear SVC (Tuned)': results_svc_tuned\n",
    "}\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "results_data = []\n",
    "for name, res in all_results.items():\n",
    "    results_data.append({\n",
    "        'Model': name,\n",
    "        'Train_Accuracy': res['train_accuracy'],\n",
    "        'Train_Precision': res['train_precision'],\n",
    "        'Train_Recall': res['train_recall'],\n",
    "        'Train_F1': res['train_f1'],\n",
    "        'Train_FPR': res['train_fpr'],\n",
    "        'Test_Accuracy': res['test_accuracy'],\n",
    "        'Test_Precision': res['test_precision'],\n",
    "        'Test_Recall': res['test_recall'],\n",
    "        'Test_F1': res['test_f1'],\n",
    "        'Test_FPR': res['test_fpr'],\n",
    "        'Training_Time_sec': res['training_time_sec'],\n",
    "        'Test_Time_sec': res['test_time_sec'],\n",
    "        'Avg_Latency_ms': res['avg_latency_ms'],\n",
    "        'Memory_MB': res['memory_mb'],\n",
    "        'Model_Size_MB': res['model_size_mb'],\n",
    "        'Throughput': res['throughput_samples_per_sec'],\n",
    "        'Cost_Effectiveness': res['cost_effectiveness'],\n",
    "        'Performance_per_MB': res['performance_per_mb']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE RESULTS SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HYPERPARAMETER TUNING IMPACT ON TEST SET\n",
      "======================================================================\n",
      "\n",
      "          Algorithm  Baseline_Test_F1  Tuned_Test_F1  Improvement  Improvement_Percent\n",
      "Logistic Regression             84.12          84.48         0.36                 0.43\n",
      "                KNN             86.91          88.35         1.44                 1.66\n",
      "         Linear SVC             83.64          86.32         2.68                 3.20\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate improvements from tuning\n",
    "improvements = []\n",
    "\n",
    "for algo in ['Logistic Regression', 'KNN', 'Linear SVC']:\n",
    "    base_f1 = results_df[results_df['Model'] == f\"{algo} (Baseline)\"]['Test_F1'].values[0]\n",
    "    tuned_f1 = results_df[results_df['Model'] == f\"{algo} (Tuned)\"]['Test_F1'].values[0]\n",
    "    improvement = tuned_f1 - base_f1\n",
    "    \n",
    "    improvements.append({\n",
    "        'Algorithm': algo,\n",
    "        'Baseline_Test_F1': base_f1,\n",
    "        'Tuned_Test_F1': tuned_f1,\n",
    "        'Improvement': improvement,\n",
    "        'Improvement_Percent': round((improvement / base_f1) * 100, 2)\n",
    "    })\n",
    "\n",
    "improvements_df = pd.DataFrame(improvements)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETER TUNING IMPACT ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\" + improvements_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING MODEL AND EVALUATING ON TEST SET\n",
      "======================================================================\n",
      "\n",
      "Loading model from: 123SVC.pkl\n",
      "✓ Model loaded successfully\n",
      "\n",
      "--- MODEL MEMORY USAGE ---\n",
      "Model memory size: 0.0046 MB\n",
      "File size on disk: 0.0046 MB\n",
      "\n",
      "--- MODEL INFORMATION ---\n",
      "Model type: LinearSVC\n",
      "Model parameters:\n",
      "  C: 100\n",
      "  class_weight: None\n",
      "  dual: auto\n",
      "  fit_intercept: True\n",
      "  intercept_scaling: 1\n",
      "  loss: hinge\n",
      "  max_iter: 1000\n",
      "  multi_class: ovr\n",
      "  penalty: l2\n",
      "  random_state: 42\n",
      "  tol: 0.0001\n",
      "  verbose: 0\n",
      "\n",
      "======================================================================\n",
      "TEST SET PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "--- PREDICTION MEMORY USAGE ---\n",
      "Peak memory for prediction: 110.6421 MB\n",
      "Current memory for prediction: 0.5660 MB\n",
      "Test set size: (74000, 194)\n",
      "Memory per sample: 1.5310 KB\n",
      "\n",
      "--- TEST METRICS ---\n",
      "Accuracy:  0.8711\n",
      "Precision: 0.8761\n",
      "Recall:    0.8711\n",
      "F1-Score:  0.8707\n",
      "\n",
      "--- CONFUSION MATRIX ---\n",
      "[[34363  2637]\n",
      " [ 6901 30099]]\n",
      "\n",
      "--- CLASSIFICATION REPORT ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88     37000\n",
      "           1       0.92      0.81      0.86     37000\n",
      "\n",
      "    accuracy                           0.87     74000\n",
      "   macro avg       0.88      0.87      0.87     74000\n",
      "weighted avg       0.88      0.87      0.87     74000\n",
      "\n",
      "\n",
      "======================================================================\n",
      "MEMORY SUMMARY\n",
      "======================================================================\n",
      "Model size in memory:     0.0046 MB\n",
      "Model size on disk:       0.0046 MB\n",
      "Prediction memory (peak): 110.6421 MB\n",
      "Memory per prediction:    1.5310 KB\n",
      "Total memory footprint:   110.6467 MB\n",
      "\n",
      "======================================================================\n",
      "EVALUATION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import tracemalloc\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING MODEL AND EVALUATING ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "# Load the model from .pkl file\n",
    "model_filename = '123SVC.pkl'  # Change this to your .pkl filename\n",
    "print(f\"\\nLoading model from: {model_filename}\")\n",
    "with open(model_filename, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "print(\"✓ Model loaded successfully\")\n",
    "# Get model memory usage\n",
    "def get_size_mb(obj):\n",
    "    size_bytes = sys.getsizeof(pickle.dumps(obj))\n",
    "    return size_bytes / (1024 * 1024)\n",
    "model_size_mb = get_size_mb(loaded_model)\n",
    "file_size_mb = os.path.getsize(model_filename) / (1024 * 1024)\n",
    "print(f\"\\n--- MODEL MEMORY USAGE ---\")\n",
    "print(f\"Model memory size: {model_size_mb:.4f} MB\")\n",
    "print(f\"File size on disk: {file_size_mb:.4f} MB\")\n",
    "# Display model information\n",
    "print(f\"\\n--- MODEL INFORMATION ---\")\n",
    "print(f\"Model type: {type(loaded_model).__name__}\")\n",
    "if hasattr(loaded_model, 'get_params'):\n",
    "    params = loaded_model.get_params()\n",
    "    print(f\"Model parameters:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "# Evaluate on TEST SET\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ✓ NEW: Measure prediction memory usage\n",
    "print(\"\\n--- PREDICTION MEMORY USAGE ---\")\n",
    "tracemalloc.start()\n",
    "\n",
    "# Baseline memory\n",
    "baseline_current, baseline_peak = tracemalloc.get_traced_memory()\n",
    "\n",
    "# Make predictions\n",
    "y_test_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# Memory after prediction\n",
    "pred_current, pred_peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "# Calculate memory used for prediction\n",
    "pred_memory_mb = (pred_peak - baseline_peak) / (1024 * 1024)\n",
    "pred_current_mb = (pred_current - baseline_current) / (1024 * 1024)\n",
    "\n",
    "print(f\"Peak memory for prediction: {pred_memory_mb:.4f} MB\")\n",
    "print(f\"Current memory for prediction: {pred_current_mb:.4f} MB\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(f\"Memory per sample: {(pred_memory_mb / len(X_test)) * 1024:.4f} KB\")\n",
    "\n",
    "# Calculate metrics\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "print(f\"\\n--- TEST METRICS ---\")\n",
    "print(f\"Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall:    {test_recall:.4f}\")\n",
    "print(f\"F1-Score:  {test_f1:.4f}\")\n",
    "# ROC-AUC (if model supports probability predictions)\n",
    "if hasattr(loaded_model, 'predict_proba'):\n",
    "    print(\"\\n--- PROBABILITY PREDICTION MEMORY ---\")\n",
    "    tracemalloc.start()\n",
    "    baseline_current, baseline_peak = tracemalloc.get_traced_memory()\n",
    "    \n",
    "    y_test_proba = loaded_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    proba_current, proba_peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    \n",
    "    proba_memory_mb = (proba_peak - baseline_peak) / (1024 * 1024)\n",
    "    print(f\"Peak memory for predict_proba: {proba_memory_mb:.4f} MB\")\n",
    "    \n",
    "    test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    print(f\"\\nROC-AUC:   {test_auc:.4f}\")\n",
    "# Confusion Matrix\n",
    "print(\"\\n--- CONFUSION MATRIX ---\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(cm)\n",
    "# Classification Report\n",
    "print(\"\\n--- CLASSIFICATION REPORT ---\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "# ✓ NEW: Summary of all memory metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MEMORY SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model size in memory:     {model_size_mb:.4f} MB\")\n",
    "print(f\"Model size on disk:       {file_size_mb:.4f} MB\")\n",
    "print(f\"Prediction memory (peak): {pred_memory_mb:.4f} MB\")\n",
    "print(f\"Memory per prediction:    {(pred_memory_mb / len(X_test)) * 1024:.4f} KB\")\n",
    "print(f\"Total memory footprint:   {model_size_mb + pred_memory_mb:.4f} MB\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING MODEL AND EVALUATING ON TEST SET\n",
      "======================================================================\n",
      "\n",
      "Loading model from: 123Logistic.pkl\n",
      "✓ Model loaded successfully\n",
      "\n",
      "--- MODEL MEMORY USAGE ---\n",
      "Model memory size: 0.0047 MB\n",
      "File size on disk: 0.0047 MB\n",
      "\n",
      "--- MODEL INFORMATION ---\n",
      "Model type: LogisticRegression\n",
      "Model parameters:\n",
      "  C: 100\n",
      "  class_weight: None\n",
      "  dual: False\n",
      "  fit_intercept: True\n",
      "  intercept_scaling: 1\n",
      "  l1_ratio: None\n",
      "  max_iter: 1000\n",
      "  multi_class: deprecated\n",
      "  n_jobs: -1\n",
      "  penalty: l2\n",
      "  random_state: 42\n",
      "  solver: liblinear\n",
      "  tol: 0.0001\n",
      "  verbose: 0\n",
      "  warm_start: False\n",
      "\n",
      "======================================================================\n",
      "TEST SET PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "--- PREDICTION MEMORY USAGE ---\n",
      "Peak memory for prediction: 110.6421 MB\n",
      "Current memory for prediction: 0.5661 MB\n",
      "Test set size: (74000, 194)\n",
      "Memory per sample: 1.5310 KB\n",
      "\n",
      "--- TEST METRICS ---\n",
      "Accuracy:  0.8285\n",
      "Precision: 0.8435\n",
      "Recall:    0.8285\n",
      "F1-Score:  0.8266\n",
      "\n",
      "--- PROBABILITY PREDICTION MEMORY ---\n",
      "Peak memory for predict_proba: 110.6591 MB\n",
      "\n",
      "ROC-AUC:   0.9573\n",
      "\n",
      "--- CONFUSION MATRIX ---\n",
      "[[26790 10210]\n",
      " [ 2478 34522]]\n",
      "\n",
      "--- CLASSIFICATION REPORT ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.72      0.81     37000\n",
      "           1       0.77      0.93      0.84     37000\n",
      "\n",
      "    accuracy                           0.83     74000\n",
      "   macro avg       0.84      0.83      0.83     74000\n",
      "weighted avg       0.84      0.83      0.83     74000\n",
      "\n",
      "\n",
      "======================================================================\n",
      "MEMORY SUMMARY\n",
      "======================================================================\n",
      "Model size in memory:     0.0047 MB\n",
      "Model size on disk:       0.0047 MB\n",
      "Prediction memory (peak): 110.6421 MB\n",
      "Memory per prediction:    1.5310 KB\n",
      "Total memory footprint:   110.6468 MB\n",
      "\n",
      "======================================================================\n",
      "EVALUATION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import tracemalloc\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING MODEL AND EVALUATING ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "# Load the model from .pkl file\n",
    "model_filename = '123Logistic.pkl'  # Change this to your .pkl filename\n",
    "print(f\"\\nLoading model from: {model_filename}\")\n",
    "with open(model_filename, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "print(\"✓ Model loaded successfully\")\n",
    "# Get model memory usage\n",
    "def get_size_mb(obj):\n",
    "    size_bytes = sys.getsizeof(pickle.dumps(obj))\n",
    "    return size_bytes / (1024 * 1024)\n",
    "model_size_mb = get_size_mb(loaded_model)\n",
    "file_size_mb = os.path.getsize(model_filename) / (1024 * 1024)\n",
    "print(f\"\\n--- MODEL MEMORY USAGE ---\")\n",
    "print(f\"Model memory size: {model_size_mb:.4f} MB\")\n",
    "print(f\"File size on disk: {file_size_mb:.4f} MB\")\n",
    "# Display model information\n",
    "print(f\"\\n--- MODEL INFORMATION ---\")\n",
    "print(f\"Model type: {type(loaded_model).__name__}\")\n",
    "if hasattr(loaded_model, 'get_params'):\n",
    "    params = loaded_model.get_params()\n",
    "    print(f\"Model parameters:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "# Evaluate on TEST SET\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ✓ NEW: Measure prediction memory usage\n",
    "print(\"\\n--- PREDICTION MEMORY USAGE ---\")\n",
    "tracemalloc.start()\n",
    "\n",
    "# Baseline memory\n",
    "baseline_current, baseline_peak = tracemalloc.get_traced_memory()\n",
    "\n",
    "# Make predictions\n",
    "y_test_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# Memory after prediction\n",
    "pred_current, pred_peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "# Calculate memory used for prediction\n",
    "pred_memory_mb = (pred_peak - baseline_peak) / (1024 * 1024)\n",
    "pred_current_mb = (pred_current - baseline_current) / (1024 * 1024)\n",
    "\n",
    "print(f\"Peak memory for prediction: {pred_memory_mb:.4f} MB\")\n",
    "print(f\"Current memory for prediction: {pred_current_mb:.4f} MB\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(f\"Memory per sample: {(pred_memory_mb / len(X_test)) * 1024:.4f} KB\")\n",
    "\n",
    "# Calculate metrics\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "print(f\"\\n--- TEST METRICS ---\")\n",
    "print(f\"Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall:    {test_recall:.4f}\")\n",
    "print(f\"F1-Score:  {test_f1:.4f}\")\n",
    "# ROC-AUC (if model supports probability predictions)\n",
    "if hasattr(loaded_model, 'predict_proba'):\n",
    "    print(\"\\n--- PROBABILITY PREDICTION MEMORY ---\")\n",
    "    tracemalloc.start()\n",
    "    baseline_current, baseline_peak = tracemalloc.get_traced_memory()\n",
    "    \n",
    "    y_test_proba = loaded_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    proba_current, proba_peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    \n",
    "    proba_memory_mb = (proba_peak - baseline_peak) / (1024 * 1024)\n",
    "    print(f\"Peak memory for predict_proba: {proba_memory_mb:.4f} MB\")\n",
    "    \n",
    "    test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    print(f\"\\nROC-AUC:   {test_auc:.4f}\")\n",
    "# Confusion Matrix\n",
    "print(\"\\n--- CONFUSION MATRIX ---\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(cm)\n",
    "# Classification Report\n",
    "print(\"\\n--- CLASSIFICATION REPORT ---\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "# ✓ NEW: Summary of all memory metrics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MEMORY SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model size in memory:     {model_size_mb:.4f} MB\")\n",
    "print(f\"Model size on disk:       {file_size_mb:.4f} MB\")\n",
    "print(f\"Prediction memory (peak): {pred_memory_mb:.4f} MB\")\n",
    "print(f\"Memory per prediction:    {(pred_memory_mb / len(X_test)) * 1024:.4f} KB\")\n",
    "print(f\"Total memory footprint:   {model_size_mb + pred_memory_mb:.4f} MB\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING MODEL AND EVALUATING ON TEST SET\n",
      "======================================================================\n",
      "\n",
      "Loading model from: 123knn_tuned_model.pkl\n",
      "✓ Model loaded successfully\n",
      "\n",
      "--- MEMORY USAGE ---\n",
      "Model memory size: 166.6292 MB\n",
      "File size on disk: 166.6291 MB\n",
      "\n",
      "--- MODEL INFORMATION ---\n",
      "Model type: KNeighborsClassifier\n",
      "Model parameters:\n",
      "  algorithm: auto\n",
      "  leaf_size: 30\n",
      "  metric: manhattan\n",
      "  metric_params: None\n",
      "  n_jobs: -1\n",
      "  n_neighbors: 11\n",
      "  p: 2\n",
      "  weights: distance\n",
      "\n",
      "======================================================================\n",
      "TEST SET PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Accuracy:  0.8768\n",
      "Precision: 0.8819\n",
      "Recall:    0.8768\n",
      "F1-Score:  0.8764\n",
      "ROC-AUC:   0.9656\n",
      "\n",
      "--- CONFUSION MATRIX ---\n",
      "[[30318  6682]\n",
      " [ 2432 34568]]\n",
      "\n",
      "--- CLASSIFICATION REPORT ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.82      0.87     37000\n",
      "           1       0.84      0.93      0.88     37000\n",
      "\n",
      "    accuracy                           0.88     74000\n",
      "   macro avg       0.88      0.88      0.88     74000\n",
      "weighted avg       0.88      0.88      0.88     74000\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EVALUATION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING MODEL AND EVALUATING ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the model from .pkl file\n",
    "model_filename = '123knn_tuned_model.pkl'  # Change this to your .pkl filename\n",
    "\n",
    "print(f\"\\nLoading model from: {model_filename}\")\n",
    "with open(model_filename, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "print(\"✓ Model loaded successfully\")\n",
    "\n",
    "# Get model memory usage\n",
    "def get_size_mb(obj):\n",
    "    size_bytes = sys.getsizeof(pickle.dumps(obj))\n",
    "    return size_bytes / (1024 * 1024)\n",
    "\n",
    "model_size_mb = get_size_mb(loaded_model)\n",
    "file_size_mb = os.path.getsize(model_filename) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n--- MEMORY USAGE ---\")\n",
    "print(f\"Model memory size: {model_size_mb:.4f} MB\")\n",
    "print(f\"File size on disk: {file_size_mb:.4f} MB\")\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\n--- MODEL INFORMATION ---\")\n",
    "print(f\"Model type: {type(loaded_model).__name__}\")\n",
    "if hasattr(loaded_model, 'get_params'):\n",
    "    params = loaded_model.get_params()\n",
    "    print(f\"Model parameters:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Evaluate on TEST SET\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "y_test_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nAccuracy:  {test_acc:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall:    {test_recall:.4f}\")\n",
    "print(f\"F1-Score:  {test_f1:.4f}\")\n",
    "\n",
    "# ROC-AUC (if model supports probability predictions)\n",
    "if hasattr(loaded_model, 'predict_proba'):\n",
    "    y_test_proba = loaded_model.predict_proba(X_test)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    print(f\"ROC-AUC:   {test_auc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n--- CONFUSION MATRIX ---\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(cm)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n--- CLASSIFICATION REPORT ---\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
